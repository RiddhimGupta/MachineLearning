{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RiddhimGupta/MachineLearning/blob/master/Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irrYYiP6cVoW",
        "colab_type": "code",
        "outputId": "abcdaee1-560f-4250-8e12-097e0c138443",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAKh1I9ydCLf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "angry_data = 'https://drive.google.com/open?id=1N8T08pBdI_0XQ2oChtrN-MFyXLCnOYNG'\n",
        "happy_data = 'https://drive.google.com/open?id=1Ps_GGkEyOdMWCguXIXdpY0B9lmTxNz9i'\n",
        "neutral_data = 'https://drive.google.com/open?id=1diC6inuoG2odZg9jbO8yO5f3NwBWQ7X9'\n",
        "sad_data = 'https://drive.google.com/open?id=1zXYiXi3MrsmPN5trkBg4lvDLlaAi9vHf'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cx4Aun7pbi7N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##############################################\n",
        "#CAPTURE IMAGE#\n",
        "##############################################\n",
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "\n",
        "def take_photo(filename='photo.jpg', quality=0.8):\n",
        "  js = Javascript('''\n",
        "    async function takePhoto(quality) {\n",
        "      const div = document.createElement('div');\n",
        "      const capture = document.createElement('button');\n",
        "      capture.textContent = 'Capture';\n",
        "      div.appendChild(capture);\n",
        "\n",
        "      const video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "\n",
        "      document.body.appendChild(div);\n",
        "      div.appendChild(video);\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      // Resize the output to fit the video element.\n",
        "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "      // Wait for Capture to be clicked.\n",
        "      await new Promise((resolve) => capture.onclick = resolve);\n",
        "\n",
        "      const canvas = document.createElement('canvas');\n",
        "      canvas.width = video.videoWidth;\n",
        "      canvas.height = video.videoHeight;\n",
        "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "      stream.getVideoTracks()[0].stop();\n",
        "      div.remove();\n",
        "      return canvas.toDataURL('image/jpeg', quality);\n",
        "    }\n",
        "    ''')\n",
        "  display(js)\n",
        "  data = eval_js('takePhoto({})'.format(quality))\n",
        "  binary = b64decode(data.split(',')[1])\n",
        "  with open(filename, 'wb') as f:\n",
        "    f.write(binary)\n",
        "  return filename"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdWTPcGdcUKs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "3d9c4d3d-0937-4394-8eea-a165b32a616b"
      },
      "source": [
        "!pip install pypiwin32"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pypiwin32\n",
            "  Using cached https://files.pythonhosted.org/packages/d0/1b/2f292bbd742e369a100c91faa0483172cd91a1a422a6692055ac920946c5/pypiwin32-223-py3-none-any.whl\n",
            "Collecting pywin32>=223 (from pypiwin32)\n",
            "\u001b[31m  ERROR: Could not find a version that satisfies the requirement pywin32>=223 (from pypiwin32) (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for pywin32>=223 (from pypiwin32)\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AeNXy75SVX7T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "outputId": "d0d43e09-673c-4edf-b6ea-28ee2e05d032"
      },
      "source": [
        "from time import sleep\n",
        "def recognize_emotion():\n",
        "    predictions = []\n",
        "    confidence = []\n",
        "    pred, conf = fishface.predict(facedict[x])\n",
        "    cv2.imwrite(\"images\\\\%s.jpg\" %x, facedict[x])\n",
        "    predictions.append(pred)\n",
        "    confidence.append(conf)\n",
        "    print(\"I think you're %s\" %emotions[max(set(predictions), key=predictions.count)])\n",
        "    \n",
        "    mp = Dispatch(\"WMPlayer.OCX\")\n",
        "    if pred == 0:\n",
        "        tune = mp.newMedia(\"path of / the / song /angry.mp3\")\n",
        "        print(\"Angry song is playing.\")\n",
        "    elif pred == 1:\n",
        "        tune = mp.newMedia(\"path of / the / song /happy.mp3\")\n",
        "        print(\"Happy song is playing.\")\n",
        "    elif pred == 2:\n",
        "        tune = mp.newMedia(\"path of / the / song /sad.mp3\")\n",
        "        print(\"Sad song is playing.\")\n",
        "    elif pred == 3:\n",
        "        tune = mp.newMedia(\"path of / the / song /nuetral.mp3\")\n",
        "        print(\"Nuetral song is playing.\")\n",
        "    elif pred == 4:\n",
        "        tune = mp.newMedia(\"path of / the / song /surprise.mp3\")\n",
        "        print(\"Surprise song is playing.\")\n",
        "\n",
        "    mp.currentPlaylist.appendItem(tune)\n",
        "    mp.controls.play()\n",
        "    sleep(1)\n",
        "    mp.controls.playItem(tune)\n",
        "    # to stop playing use\n",
        "    input(\"Press Enter to stop playing\")\n",
        "    mp.controls.stop()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-da12c15fb686>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mwin32com\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msleep\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrecognize_emotion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mconfidence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'win32com'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2Fk3xSj4Bj0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "01f3a41c-9b12-4115-ab5d-3cf336272650"
      },
      "source": [
        "import cv2\n",
        "import argparse\n",
        "from IPython.display import Image\n",
        "import numpy\n",
        "\n",
        "# We load the xml file\n",
        "classifier = cv2.CascadeClassifier('haarcascade_frontalface_alt.xml')\n",
        "fishface = cv2.face.FisherFaceRecognizer_create()\n",
        "try:\n",
        "    fishface.read(\"trained_emoclassifier.xml\")\n",
        "except:\n",
        "    print(\"no xml found. Using --update will create one.\")\n",
        "parser = argparse.ArgumentParser(description=\"Options for the emotion-based music player\") #Create parser object\n",
        "parser.add_argument(\"--update\", help=\"Call to grab new images and update the model accordingly\", action=\"store_true\") #Add --update argument\n",
        "args = parser.parse_args() #Store any given arguments in an object\n",
        "try:\n",
        "  filename = take_photo()\n",
        "  print('Saved to {}'.format(filename))\n",
        "  \n",
        "  # Show the image which was just taken.\n",
        "  display(Image(filename))\n",
        "except Exception as err:\n",
        "  # Errors will be thrown if the user does not have a webcam or if they do not\n",
        "  # grant the page permission to access it.\n",
        "  print(str(err))\n",
        "\n",
        "from PIL import Image\n",
        "img = Image.open(\"photo.jpg\")\n",
        "transposed_img = img.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "transposed_img.save('photo.jpg')\n",
        "img = Image.open(\"photo.jpg\")\n",
        "im = numpy.asarray(img)\n",
        "    \n",
        "    # Resize the image to speed up detection\n",
        "    #mini = cv2.resize(im, (int(im.shape[1]/size), int(im.shape[0]/size)))\n",
        "gray = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)\n",
        "clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
        "clahe_image = clahe.apply(gray)\n",
        " #To run classifier \n",
        "face = classifier.detectMultiScale(clahe_image, scaleFactor=1.1, minNeighbors=15, minSize=(10, 10), flags=cv2.CASCADE_SCALE_IMAGE)\n",
        "#To draw rectangle around detected face\n",
        "for (x, y, w, h) in face:\n",
        "    cv2.rectangle(im, (x, y), (x+w, y+h), (0, 0, 255), 2)    \n",
        "    #Save just the rectangle faces in SubRecFaces\n",
        "    faceslice = im[y:y+h, x:x+w]\n",
        "    #draw it on \"frame\", (coordinates), (size), (RGB color), thickness 2    \n",
        "    cv2.imwrite(\"test.jpg\", faceslice)\n",
        "    text =main(\"test.jpg\")\n",
        "    text = text.title()# Title Case looks Stunning.\n",
        "    font = cv2.FONT_HERSHEY_TRIPLEX\n",
        "    cv2.putText(im, text,(x+w,y), font, 1, (0,0,255), 2)\n",
        "    # Show the image\n",
        "cv2.imwrite(\"capture.jpg\", im)\n",
        "#key = cv2.waitKey(10)\n",
        "    \n",
        "    # if Esc key is press then break out of the loop\n",
        "#if key == 27:#The Esc key\n",
        "  # break\n"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "no xml found. Using --update will create one.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "usage: ipykernel_launcher.py [-h] [--update]\n",
            "ipykernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-97b921a7-4c98-459c-8ecb-da0e9019ce34.json\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tohKKtgW4TtR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}